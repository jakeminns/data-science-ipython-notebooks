{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80ff02a",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49001492",
   "metadata": {},
   "source": [
    "Simply put, feature selection is the process for reducing the number of predictors used to train a machine learning model. While in theory it may seem intuitive to provide your model with all the information you have, and allow the model to determine the importance of your data iteratively through the learning process. In practice, we find the quantity of data required for a model to generalise over grows exponentially as the number of features or dimensions increases, this is know colloquially as the curse of dimensionality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6ed99",
   "metadata": {},
   "source": [
    "It’s often the case that our training dataset contains features that are more important than others, and some irrelevant entirely. Especially when using packages like TsFresh. In general, feature selection can be achieved either by filtering, where prior to feeding data through a model features are removed using some kind of statistical modeling. Or through wrapping where model performance is evaluated through sampling of the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ba524",
   "metadata": {},
   "source": [
    "## ANOVA-F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec66ac6",
   "metadata": {},
   "source": [
    "Starting with an example from the sklearn library, the iris dataset. Where, we look to classify 3 types of irises’ (Setosa, Versicolour, and Virginica) using the features Sepal Length, Sepal Width, Petal Length and Petal Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f180b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba226e0f",
   "metadata": {},
   "source": [
    "Our first method for filtering features is known as the Analysis of variance (ANOVA), here we look at the variation/separation of the mean of feature distributions across classification classes using statistical F-tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6a562",
   "metadata": {},
   "source": [
    "To start, an F-statistic is the ratio of two variances, where the variance is a measure of the dispersion of data from the mean.\n",
    "\n",
    "For the ANOVA-F test we compare the \"distance between class variance\" to the class variance.\n",
    "\n",
    "Intuitively we can see below how analysing the separation of class means, and the variance of classes is useful for feature selection.\n",
    "\n",
    "Below we have plotted the value of two features on the x and y axis for two classes coloured blue and orange, the data has then been projected onto each axis to visualise the separation of classes. Here, it is clear that y is a better separator than x because there is less overlap between the distributions of both classes (greater separation of means and less compactness of classes). Specifically, according to the y feature, the classes are far away from each other (i.e. the distance between the means of class distribution is greater). For the y feature the distributions also do not overlap, each class distribution is more compact for y than x, and so less likely to overlap (i.e. the variance of each class distribution is lower). This outlines the two variance we compare in the ANOVA-F test.\n",
    "\n",
    "distance between classes/compactness of classes\n",
    "\n",
    "Where the higher the ratio, the better the feature and separating classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af6e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_hist(x, y, ax, ax_histx, ax_histy):\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    # the scatter plot:\n",
    "    ax.scatter(x, y)\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    ax_histx.hist(x, bins=bins)\n",
    "    ax_histy.hist(y, bins=bins, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffd0f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAILCAYAAABW087vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhb0lEQVR4nO3df7Cld10f8PeHTZAFcWMmWwaSrMu0CoMkIXALtKgVUX5IkIidVBitWDpbLSoCgxPQwZWhlUoFymitq0FsBexWElSwAgWqrSPQXX4s4UcqYiBZwJDGrICrLJtv/zj37t69e3+c+z3n3nPuOa/XzM695znPOeezy8O97zzP9/N8qrUWAIDNutekCwAAdiYhAgDoIkQAAF2ECACgixABAHS5YDs/7JJLLmn79+/fzo8EYAsdPXr0ztba3knXwWRsa4jYv39/jhw5sp0fCcAWqqpPT7oGJsflDACgixABAHQRIgCALkIEANBlWxdWAuxk+69/23nbbn3FUydQCUwHZyIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAumwYIqrqdVV1R1XdvGzbwao6XlUfWvzz3VtbJgAwbYY5E/H6JE9eZfurW2uPWPzzB+MtCwCYdhuGiNbaHye5axtqAQB2kAtGeO2PVdU/T3IkyQtba3+12k5VdSDJgSTZt2/fCB8HsIMc3LPG9hPbWwdsod6Flb+S5O8neUSSzyX5xbV2bK0daq0ttNYW9u7d2/lxAMC06QoRrbW/bK2dbq3dk+TXkjx6vGUBANOuK0RU1QOXPfzeJDevtS8AMJs2XBNRVW9K8u1JLqmq25P8bJJvr6pHJGlJbk3yr7auRABgGm0YIlprz1xl8w1bUAsAsIO4YyUA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOiyYYioqtdV1R1VdfOybRdX1Tur6s8Wv3791pYJAEybYc5EvD7Jk1dsuz7Ju1pr35jkXYuPAYA5smGIaK39cZK7Vmx+epLfXPz+N5NcO96yAIBpd0Hn6x7QWvvc4vefT/KAtXasqgNJDiTJvn37Oj8OYMYd3LPG9hPbWwdswsgLK1trLUlb5/lDrbWF1trC3r17R/04AGBK9IaIv6yqBybJ4tc7xlcSALAT9IaI30vyQ4vf/1CS3x1POQDATjFMi+ebkvxpkodU1e1V9Zwkr0jyXVX1Z0m+c/ExADBHNlxY2Vp75hpPPWHMtQAAO0hvdwbAzNp//dsmXQLsCG57DQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAubnsNzC23t4bROBMBAHQRIgCALkIEANBFiAAAuggRAEAX3RkAI1irw+PW+2zxBx/cs8b2E1v8wXCWMxEAQBchAgDoMtLljKq6NckXk5xO8tXW2sI4igIApt841kQ8vrV25xjeBwDYQVzOAAC6jHomoiV5R1W1JL/aWju0coeqOpDkQJLs27dvxI8D2OHW6qoY1/6wjUY9E/EtrbVHJnlKkudW1bet3KG1dqi1ttBaW9i7d++IHwcATIuRQkRr7fji1zuS3JTk0eMoCgCYft0hoqruV1X3X/o+yROT3DyuwgCA6TbKmogHJLmpqpbe542ttT8cS1UAwNTrDhGttU8luWqMtQBsibVuTb1Zt97nWee/99++cSzvDTuRFk8AoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuowzgAmDaHNyzyrYT218Hc8GZCACgixABAHQRIgCALkIEANBFiAAAuujOAFjh1vs8a0v2nZjVOjYSXRuMzJkIAKDLSCGiqp5cVbdU1Ser6vpxFQUATL/uEFFVu5L8cpKnJHlYkmdW1cPGVRgAMN1GORPx6CSfbK19qrX2lSS/neTp4ykLAJh2oyysvDTJX1bV7yR5eJL7J/nTsVQFAEy9UbszHpvkF1pr/7Sqnp3kH6/coaoOJDmw+PBLVXXLiJ85CZckuXPSRXRS+2SofTLGUnuNoZAO2//v/nNj+Zt+wzjehJ1plBBxd5LLktyw+PiBSf5i5U6ttUNJDo3wORNXVUdaawuTrqOH2idD7ZOhdtheo4SIv8pgTcV/q6p/kOTyJE9audPyMxH3u9/9HvXQhz50hI+cjPve975ZWFhok66jh9onQ+2Tofbtd/To0S8luSXZuT/jOd/Ro0fvbK3t3Wi/UUJEJbkwgwWWpzI4iJ6e5MjynZafiVhYWGhHjhzJTrOwsJCdWHei9klR+2SofftV1S1LZ1B26s94zldVnx5mv1FCxO1Jbmut7V/8wG9NMpP3ijhw4MDGO00ptU+G2idD7bC9qrX+s2dV9b+S/MvW2i1VdTDJ/VprL1prfykVYLZU1VFnImbP8v9d1zNqd8aPJ3lDVd07yaeS/PCI7wcA7BAjhYjW2oeSWE0MAHPIAC4AoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgywWTLgCAGfHZDyYH90y6itUdPDHpCmaSMxEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEwCQdO5y8+uHJwYsGX48dnnRFMDT3iQCYlGOHk9//ieTUycHjE7cNHifJlddNri4YkjMRAJPyrpedDRBLTp0cbIcdQIgAmJQTt29uO0wZIQJgUvZctrntMGWECIBJecJLkwt3n7vtwt2D7bADjLSwsqpuTfLFJKeTfLW1tjCOogDmwtLiyXe9bHAJY89lgwCxmUWVxw6P9noYwTi6Mx7fWrtzDO8DMH+uvK7/l77uDibM5QyAnUp3BxM26pmIluQdVdWS/Gpr7dDKHarqQJIDSbJv374RPw6AM6aju+OSqjqSJPv21HZ+7uYc3DPpClZ38MSkKxjJqGcivqW19sgkT0ny3Kr6tpU7tNYOtdYWWmsLe/fuHfHjADhjOro77jzzM/6+Uxwi2BIjhYjW2vHFr3ckuSnJo8dRFABD0N3BhHWHiKq6X1Xdf+n7JE9McvO4CgOYexvN1bjyuuRpr032XJ6kBl+f9trBc+ZxsA1GWRPxgCQ3VdXS+7yxtfaHY6kKYN4N23mxsrtDxwbbqPtMRGvtU621qxb/fHNr7d+MszCAudbbeaFjg22kxRNgGvV2XkxHxwZzQogAmEa9nRfT0bHBnBAiAKZRb+eFjg22kRABMAm9nRfrLY5cmqNx6mRSuwbbhnkddBrH7AwANqO382Iz79lOnz0DIUCwRZyJANhuW9FBoSuDCRAiALbbVnRQ6MpgAoQIgO22FR0UujKYACECYLttRQeFrgwmwMJKgO22tNDxXS8bXG7Yc9noCyC34j3ZetM6onxIQgTAJKzsvHjrC5KbfmTQVVG7kkc9O9n32I1DwVJb52r7LLWRChVsESECYNLe+oLkyA1nH7fTg8dHfiPJPYNtq7WBrtcqmhjExZazJgJg0o6+fo0n7jn34cqWzfXaOrV8sg2ciQCYtHZ6+H2Xt2z2tHVq+WSMnIkAmLSlW1QPY3nL5nptnVo+2QZCBMCkPerZazyx4kf0ypbN9do6tXyyDVzOAJi0a16V/L9PJn/xR2e3PfifJFf/wPmdF8lix8VtgzMYS90c7fRg2NbKDgwtn2whIQJg0o4dTm5//7nbbn//IEQ8/+Zz91s5ZGvp62rDtjYzwAs6uJwBMGnDdlKstt96+8MWEyIAJm3YLouNOit0XrDNhAiASRu2k2KjzgqdF2wzIQJg0obtpFhtv/X2hy1mYSXAOKw2wyJZ3Hbb2h0Ub33B4I6Vy284tfviwdcbDwxe/41PTP7sHYP33v31yQW7k5N3rd+VAdtAiAAY1WozLH73uUlryT2nBtuWQsLyGRafee+5MzOWnLw758zMWL7PybsGZx2e8WtCAxPncgbAqFbrmjj9lbMBYqWlTophZ2as9XqYMGciAEbV0xVx4vYkbXs/E8bMmQiAUfV0Rey5bHMzM8bxmTBmQgTAqFbrmth17+ReF66+/1InxbAzM9Z6PUyYyxkAo1pa4Li8E+P0V852WazVSbH0uqXujNo1CBb7Hnt+V8fK1yfnz9DQpcE2EyIAxmHpF/fyLo1hOimuedXgz2pWzslYfgZitRkayzs/BAm2gcsZAOMy7AyMUd/LDA2mhDMRAOMy7AyMrX4vnRtsE2ciAMZl2BkYo76XGRpMCSECYFyGnYEx6nuZocGUECIAxuXK65KnvXbQJZEafH3aaze/yHFpDsepk2fvJVG7zl3vcOZzcnaf3s+DTtZEAIzT8tbNHivncCx1XqzswHjaa5Pn3zxarTAiZyIApsl6nRdLdGAwJYQIgGkybGeFDgymgBABME2G7azQgcEUECIApsl6nRdLdGAwJYQIgGmyWofHwnNG7/iALaA7A2CrLLVqnrjt7LbzhmzdPrg0sXxw1qgdHrBNhAiArbCyVXNJO50cuSE58htJ7hlsMziLHcrlDICtsGGr5j3nPtS2yQ4kRABshXEO3YIpJUQAbIVxDt2CKSVEAGyFDVs1V/z41bbJDiREAGyFK69LrnrW2eFYK+2+KNl9cbRtspPpzgDYCscOJx9+49nBWSudvGtw9uEZh4QHdixnIgC2gkFazAEhAmArGKTFHBAiALaCQVrMASECYCsYpMUcGHlhZVXtSnIkyfHW2jWjlwSwg731BcnR1w8WVNa9kgvvl5z68qBLY/kiy9o16N5Iklc/fHDr66V99lx+7iwNmFLjOBPxvCQfH8P7AOxsb33BYC7GUlho9wwCxMJzku/9T+eemWinkw/+l+Qt//rsgK6l1y3N0jh2eHvrh00aKURU1WVJnprk18dTDsAOdvT1a29frVvj9FeSe06t/hqdG+wAo17OeE2Sn0py/7V2qKoDSQ4kyb59+0b8OIApttY9IdrpWZ6lcUlVHUmSfXtq0rXsPAdPTLqC1f3ccP9bdp+JqKprktzRWju63n6ttUOttYXW2sLevXt7Pw5g+q11d8raNcuzNO488zP+vkLEvBnlcsbjknxPVd2a5LeTfEdV/dZYqgLYiR717LW3r9atseveyb0uXP01OjfYAbpDRGvtxa21y1pr+5N8f5J3t9Z+YGyVAUyrY4cHHRUHLxp8XVoAec2rBosol85ILHVnHHndYH3DZY9e9tyu5OofTK79j4NujKVtiVka7BhmZwBsxrHDg86JpUWSS50UyeCX/jWvGvw5s9+Xz+631IWRDNZJfPiNyb7HJs+/eXv/DjAmY7nZVGvtf7pHBDAXVuuyWK2TwuwM5oA7VgJsxlodEyu3m53BHBAiADZjrY6JldvNzmAOCBEAm7Fal8VqnRRmZzAHhAiAzbjyukHnxJ7Lk9TanRSr7bfwnI1fBzuI7gyAzbryurV/+R87PFgseeL2waUKg7SYYUIEwLhs1P4JM8blDIBxGbb9E2aEEAEwLsO2f8KMECIAxmXY9k+YEUIEwLgM2/4JM0KIABiXK69LrnrWuUO2rnqWRZXMLCECYFyOHR4M1WqnB4+XhmwtTfmEGSNEAIyL7gzmjBABMC66M5gzQgTAuOjOYM4IEQDjojuDOSNEAIzLyqFbuy9OLtid3HggefXDLbBk5ggRAON05XXJ829OnnEo+erJ5ORdSdrZORqCBDNEiADYCjo1mANCBMBW0KnBHBAiALaCTg3mgBABsBV0ajAHhAiAjRw7POiuOHjR8F0W53RqZDBHY2lNhMWVzIgLJl0AwFQ7dnjQVbG0SHKpyyLZeLDW0vO9r4cp50wEwHpG7bLQpcEMcyYCYD2jdlno0pgOB09MuoKZ5EwEwHpG7bLQpcEMEyIA1jNql4UuDWaYEAGwnpXzMPZcPng87KLIUV8PU8yaCICNXHndaL/0R309TClnIgCALkIEANBFiAAAuggRAEAXIQIA6KI7A2A1xw4Pbk194vZk99cPtp38q8FNop7wUt0WECEC4Hwrh26dvOvscwZowRkuZwCstNrQrOUM0IIkQgTA+YYZjmWAFggRAOcZZjiWAVogRACcZ7WhWcsZoAVJLKwEON/SgkndGZNz8MSkK2AIQgTAagzNgg25nAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALt0hoqruU1Xvr6oPV9VHq+rnxlkYADDdRjkT8XdJvqO1dlWSRyR5clU9dixVAUyjY4eTVz88OXjR4Ouxw5OuCCaqewBXa60l+dLiwwsX/7RxFAUwdY4dTn7/J5JTJwePT9w2eJwY1MXcGmlNRFXtqqoPJbkjyTtba+8bS1UA0+ZdLzsbIJacOjnYDnNqpFHgrbXTSR5RVRcluamqHt5au3n5PlV1IMmBJNm3b98oHwcwOSdu39z2+XFJVR1JFn/GH/z0pOthG42lO6O1dneS9yR58irPHWqtLbTWFvbu3TuOjwPYfnsu29z2+XGnn/Hza5TujL2LZyBSVbuTfFeST4ypLoDp8oSXJhfuPnfbhbsH22FOjXI544FJfrOqdmUQRg631t46nrIApszS4sl3vWxwCWPPZYMAYVElc2yU7oxjSa4eYy0A0+3K64QGWMYdKwGALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdukNEVV1eVe+pqo9V1Uer6nnjLAwAmG4XjPDaryZ5YWvtA1V1/yRHq+qdrbWPjak2AGCKdZ+JaK19rrX2gcXvv5jk40kuHVdhAMB0G+VMxBlVtT/J1Unet8pzB5IcSJJ9+/aN4+MAmB6XVNWRJNn1dXuz//q3TbqeVd36iqdOuoSZNPLCyqr62iRvTvKTrbW/Xvl8a+1Qa22htbawd+/eUT8OgOly59LP+F333TPpWthmI4WIqrowgwDxhtbajeMpCQDYCbovZ1RVJbkhycdba68aX0kAk/OWDx7PK99+Sz5798k86KLdedGTHpJrr750w+dgHo2yJuJxSX4wyUeq6kOL217SWvuDkasCmIC3fPB4XnzjR3Ly1OkkyfG7T+bFN37kzPNrPSdIMK+6Q0Rr7X8nqTHWAjBRr3z7LWdCwpKTp07nlW+/5cz3qz0nRDCvxtKdATALPnv3yU1t3+g5mHVuew2w6EEX7V5z+3rPwbwSIgAWvehJD8nuC3eds233hbvyoic9ZN3nYF65nAGwaGltw3odGLoz4CwhAmCZa6++dM1gsN5zMI9czgAAuggRAEAXIQIA6CJEAABdhAgAoIvuDIAhrTeA62fe8pG86X235XRr2VWVZz7m8rz82is2fB3bY//1b5t0CTNJiAAYwnrDuY58+q781ns/c2bf062debzwDRcb3MXMEiIAhrDecK7Pn/jbVV/zpvfdlvd84gsGdzGzhAiAIaw3nKut8ZrTrXUN9YKdwsJKgCGsN4BrV9Wqz+2qMriLmSZEAAxhvQFcz3zM5au+5pmPudzgLmaayxnA3FvePbH7wnvl5FfvSWs502XxF1/4Uv7kz+865zW7qvJ9j7r0nHkaa3VnJAZ3MZuECGCurey6+JtT95x5bnmXxUqnW8ubjx7PwjdcnGuvvjQvv/aKc0LDcgZ3MatczgDm2mpdF8Na6rKAeSVEAHNt1C4JXRbMMyECmGujdknosmCeCRHAXFute2JYuiyYdxZWAnNrqSvj5KnT2VWV063lvsu6MzZy8tTpPP+/fig/fdNH8uWvnF1XYXYG80KIAObSyq6M061l94W78m+fccV5szDW05JzAsTSe5mdwTwQIoC51DMLY7PMzmDWCRHAXOqZhbFZZmcw6yysBOZSzyyMzTI7g1knRABzqWcWxmaZncGsEyKAufCWDx7P417x7jz4+rflca94d5Lkkfv2nLPPI/ftyZFP35U3ve+2Tb3311yw+o/S93ziC0mSn3/GFbn0ot2pJJdetDs//4wrrIdgJlgTAcy8lZ0Yx+8+mRcc/lDuWbH44U/+/K7zBm0N415Vec0/e0SSrNqJ8fPPuCJ/cv13jPR3gGkkRAAzb7VOjJUBYhTLZ2joxGCeCBHAzNuOToj1PkMnBrPKmghg5m1HJ8SDLtqtE4O5I0QAM2+1Dol7jaeLM8nZbgudGMwblzOAmbe0HmHl/IqlTozTrZ2Zd5HkzLblKln1JlSXrjILw5wM5oUQAcyFI5++K58/8bdpST574mRecuOxnDx1Tx500e48/qF7855PfCFveO9n8qCLducXr7vqnF/8wwzQWtrn+N0ns6tqbHe9hGkmRAAz72fe8pFzBmq1lvzNqXuSDNowlz+3ckDWau2hKwdorTbMa619YZZYEwHMvM3ePGp5y+Z6g7qWrLbPWvvCLBEigJm3cn3DMJbaMocZoLVRC6cWT2aVEAHMvJ6BWkttmcO0bW7UwqnFk1klRAAzb7MDtZa3ZQ7TtrnaPmvtC7PEwkpg5r382iuSnG3drEp2X3Cv87ozVuu+uPbqS89rBf2+R116zkLJ5S2kS90Zp1s7r/1zmC4P5sutr3jqpEtYVf274fYTIoC58PJrrzgTJjbjLR88njcfPX5mXcXp1vLmo8ez8A0Xnxck1gsEw3R5wE7jcgbAOobpztjO94FpIkQArGOY7oztfB+YJkIEwDrGNVTLcC5mkRABsI5xDdUynItZZGElMNNG7YhYa3jXZhdDjut9YJoIEcDMGldHxEadF9v9PjAtXM4AZpaOCNhaQgQws3REwNYSIoCZpSMCtpYQAcwsHRGwtUZaWFlVr0tyTZI7WmsPH09JAOOxUUeEWRYwmlG7M16f5JeS/OfRSwEYv7U6IsyygNGNdDmjtfbHSe4aUy0A20bnBoxuy+8TUVUHkhxIkn379m31xwEMRefG2FxSVUeSwc/4aR1tzdbY8oWVrbVDrbWF1trC3r17t/rjAIaic2Ns7vQzfn7pzgDmks4NGJ3bXgNzySwLGN2oLZ5vSvLtGVwTuz3Jz7bWbhhHYQBbzSwLGM1IIaK19sxxFQIA7CzWRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6CBEAQBchAgDoIkQAAF2ECACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6CJEAABdhAgAoIsQAQB0ESIAgC5CBADQRYgAALoIEQBAFyECAOgiRAAAXYQIAKCLEAEAdBEiAIAuQgQA0EWIAAC6jBQiqurJVXVLVX2yqq4fV1EAwPTrDhFVtSvJLyd5SpKHJXlmVT1sXIUBANNtlDMRj07yydbap1prX0ny20mePp6yAIBpd8EIr700yW3LHt+e5DErd6qqA0kOLD78UlXdMsJnTsolSe6cdBGd1D4Zap8MtW+/h1TVkcXv/66qbp5oNWub1n/faa3rIcPsNEqIGEpr7VCSQ1v9OVupqo601hYmXUcPtU+G2idD7ZM1zX+Haa1tmusaZr9RLmccT3L5sseXLW4DAObAKCHi/yT5xqp6cFXdO8n3J/m98ZQFAEy77ssZrbWvVtWPJXl7kl1JXtda++jYKpsuO/lyjNonQ+2TofbJmua/w7TWtqPrqtbaVhcCAMwgd6wEALoIEQBAFyFiE6rqx6vqE1X10ar6hUnXs1lV9cKqalV1yaRrGVZVvXLx3/xYVd1UVRdNuqaN7NTbwVfV5VX1nqr62OIx/rxJ17QZVbWrqj5YVW+ddC2bVVUXVdXvLB7rH6+qfzTpmjZrGo/7aT+mp/WY3czxKEQMqaoen8EdOa9qrX1zkn8/4ZI2paouT/LEJJ+ZdC2b9M4kD2+tXZnk/yZ58YTrWdcOvx38V5O8sLX2sCSPTfLcHVR7kjwvyccnXUSn/5DkD1trD01yVXbY32OKj/tpP6an9Zgd+ngUIob3o0le0Vr7uyRprd0x4Xo269VJfirJjlpJ21p7R2vtq4sP35vB/Uim2Y69HXxr7XOttQ8sfv/FDH5wXDrZqoZTVZcleWqSX590LZtVVXuSfFuSG5KktfaV1trdEy1q86byuJ/mY3paj9nNHo9CxPC+Kcm3VtX7quqPquofTrqgYVXV05Mcb619eNK1jOhfJPnvky5iA6vdDn4qfmhtRlXtT3J1kvdNuJRhvSaDkHzPhOvo8eAkX0jyG4untn+9qu436aI2aeqP+yk8pl+T6TxmN3U8ChHLVNX/qKqbV/nz9AzuqXFxBqfEXpTkcFXVRAteZoPaX5LkpZOucS0b1L60z09ncGryDZOrdD5U1dcmeXOSn2yt/fWk69lIVV2T5I7W2tFJ19LpgiSPTPIrrbWrk3w5yVSsKZgV03ZMT/kxu6njcctnZ+wkrbXvXOu5qvrRJDe2wY013l9V92QwOOUL21XfetaqvaquyCBZfngx81yW5ANV9ejW2ue3scQ1rffvniRV9ewk1yR5Qpv+G5vs6NvBV9WFGfywfUNr7cZJ1zOkxyX5nqr67iT3SfJ1VfVbrbUfmHBdw7o9ye2ttaX/Qv6d7LwQMbXH/ZQe09N8zG7qeHQmYnhvSfL4JKmqb0py70zn5LVztNY+0lr7e621/a21/RkcII+clgCxkap6cgan/L6ntfY3k65nCDv2dvCLZ9ZuSPLx1tqrJl3PsFprL26tXbZ4fH9/kndPyQ/joSz+f/G2qlqamviEJB+bYEk9pvK4n9ZjepqP2c0ej85EDO91SV5XgzG3X0nyQzvgv4pnwS8l+Zok71w8k/Le1tqPTLakte3w28E/LskPJvlIVX1ocdtLWmt/MLmS5saPJ3nD4i/gTyX54QnXsylTfNw7pvsMfTy67TUA0MXlDACgixABAHQRIgCALkIEANBFiAAAuggRAEAXIQIA6PL/AflG7Z3iQRZaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "left, width = 0.1, 0.65\n",
    "bottom, height = 0.1, 0.65\n",
    "spacing = 0.005\n",
    "\n",
    "\n",
    "rect_scatter = [left, bottom, width, height]\n",
    "rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "# start with a square Figure\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = fig.add_axes(rect_scatter)\n",
    "ax_histx = fig.add_axes(rect_histx, sharex=ax)\n",
    "ax_histy = fig.add_axes(rect_histy, sharey=ax)\n",
    "\n",
    "\n",
    "x1 = data['data'][np.where(data['target']==0)][:,[0]]\n",
    "y1 = data['data'][np.where(data['target']==0)][:,[2]]\n",
    "# use the previously defined function\n",
    "scatter_hist(x1, y1, ax, ax_histx, ax_histy)\n",
    "\n",
    "x2 = data['data'][np.where(data['target']==1)][:,[0]]\n",
    "y2 = data['data'][np.where(data['target']==1)][:,[2]]\n",
    "# use the previously defined function\n",
    "scatter_hist(x2, y2, ax, ax_histx, ax_histy)\n",
    "\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bb2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "def binary_f_score(x1:np.array, x2:np.array) -> float:\n",
    "    \"\"\"Return the f score of a feature for a binary classficiation task.\n",
    "    \n",
    "    Args:\n",
    "      - x1: class 1 feature array\n",
    "      - x2: class 2 feature array\n",
    "    \"\"\"\n",
    "    \n",
    "    #Ratio numerator, the distance between means of each class, weighted by the population of each class.\n",
    "    mean = np.mean(np.array([ x1, x2 ])) # mean of all data\n",
    "    mean_1 = np.mean(x1) #mean of class 1\n",
    "    mean_2 = np.mean(x2) #mean of class 2\n",
    "    n_x = x1.shape[0]*(mean_1-mean)**2 + x2.shape[0]*(mean_2-mean)**2 #\n",
    "\n",
    "    #Class variance, Analogous to sample variance (https://www.onlinemathlearning.com/variance.html) Sum of squares within class divided by sum of total population.\n",
    "    SSW_1 = np.sum((x1-mean_1)**2)\n",
    "    SSW_2 = np.sum((x2-mean_1)**2)\n",
    "    d_x = (SSW_1+SSW_2)/((x1.shape[0]-1)+(x2.shape[0]-1))\n",
    "\n",
    "    return n_x/d_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a51f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature x, F-Score: 33.96472741727628\n",
      "Feature y, F-Score: 47.50746721883993\n",
      "Feature 1 Wins !\n"
     ]
    }
   ],
   "source": [
    "f_1 = binary_f_score(x1,x2)\n",
    "f_2 = binary_f_score(y1,y2)\n",
    "print(f\"Feature x, F-Score: {f_1}\")\n",
    "print(f\"Feature y, F-Score: {f_2}\")\n",
    "print(f\"Feature {np.argmax([f_1,f_2])} Wins !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb17f0b",
   "metadata": {},
   "source": [
    "## Wrapper OLS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e7e9f",
   "metadata": {},
   "source": [
    "The wrapper method uses a model dependant metric to evaluate the performance of features. This method is particularly greedy, as we are going to iteratively evaluate the performance of a model and the respective features for the entire feature space, with each run of the model we are going to remove the worst performing feature and re run the model. This continues until the value of our chosen metric for the worst performing feature is below a given threshold. This process is known as Step Back feature selection, the opposite Step Forward can also be done (although i haven’t tested this a whole lot). Due to the greedy nature of this method it is recommended to use this feature selection method following a preliminary reduction of the feature space, such as ANOVA-F or ExtreTrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b1a6e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "25bfca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0        17.99      10.38     122.80     1001.0    0.11840    0.27760   \n",
       "1        20.57      17.77     132.90     1326.0    0.08474    0.07864   \n",
       "2        19.69      21.25     130.00     1203.0    0.10960    0.15990   \n",
       "3        11.42      20.38      77.58      386.1    0.14250    0.28390   \n",
       "4        20.29      14.34     135.10     1297.0    0.10030    0.13280   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "564      21.56      22.39     142.00     1479.0    0.11100    0.11590   \n",
       "565      20.13      28.25     131.20     1261.0    0.09780    0.10340   \n",
       "566      16.60      28.08     108.30      858.1    0.08455    0.10230   \n",
       "567      20.60      29.33     140.10     1265.0    0.11780    0.27700   \n",
       "568       7.76      24.54      47.92      181.0    0.05263    0.04362   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  ...  feature_20  feature_21  \\\n",
       "0      0.30010    0.14710     0.2419    0.07871  ...      25.380       17.33   \n",
       "1      0.08690    0.07017     0.1812    0.05667  ...      24.990       23.41   \n",
       "2      0.19740    0.12790     0.2069    0.05999  ...      23.570       25.53   \n",
       "3      0.24140    0.10520     0.2597    0.09744  ...      14.910       26.50   \n",
       "4      0.19800    0.10430     0.1809    0.05883  ...      22.540       16.67   \n",
       "..         ...        ...        ...        ...  ...         ...         ...   \n",
       "564    0.24390    0.13890     0.1726    0.05623  ...      25.450       26.40   \n",
       "565    0.14400    0.09791     0.1752    0.05533  ...      23.690       38.25   \n",
       "566    0.09251    0.05302     0.1590    0.05648  ...      18.980       34.12   \n",
       "567    0.35140    0.15200     0.2397    0.07016  ...      25.740       39.42   \n",
       "568    0.00000    0.00000     0.1587    0.05884  ...       9.456       30.37   \n",
       "\n",
       "     feature_22  feature_23  feature_24  feature_25  feature_26  feature_27  \\\n",
       "0        184.60      2019.0     0.16220     0.66560      0.7119      0.2654   \n",
       "1        158.80      1956.0     0.12380     0.18660      0.2416      0.1860   \n",
       "2        152.50      1709.0     0.14440     0.42450      0.4504      0.2430   \n",
       "3         98.87       567.7     0.20980     0.86630      0.6869      0.2575   \n",
       "4        152.20      1575.0     0.13740     0.20500      0.4000      0.1625   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "564      166.10      2027.0     0.14100     0.21130      0.4107      0.2216   \n",
       "565      155.00      1731.0     0.11660     0.19220      0.3215      0.1628   \n",
       "566      126.70      1124.0     0.11390     0.30940      0.3403      0.1418   \n",
       "567      184.60      1821.0     0.16500     0.86810      0.9387      0.2650   \n",
       "568       59.16       268.6     0.08996     0.06444      0.0000      0.0000   \n",
       "\n",
       "     feature_28  feature_29  \n",
       "0        0.4601     0.11890  \n",
       "1        0.2750     0.08902  \n",
       "2        0.3613     0.08758  \n",
       "3        0.6638     0.17300  \n",
       "4        0.2364     0.07678  \n",
       "..          ...         ...  \n",
       "564      0.2060     0.07115  \n",
       "565      0.2572     0.06637  \n",
       "566      0.2218     0.07820  \n",
       "567      0.4087     0.12400  \n",
       "568      0.2871     0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(data['data'], columns=['feature_{}'.format(i) for i in range(data['data'].shape[1])])\n",
    "y = data['target']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "093a6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wrapper_OLS(X, y, feats=None, thresh=10 * 10e-7):\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    X_new = sm.add_constant(X[feats])\n",
    "\n",
    "    model = sm.OLS(y, X_new).fit()\n",
    "    model.pvalues\n",
    "\n",
    "    if feats is None:\n",
    "        top_wrapper = X.columns.tolist()\n",
    "    else:\n",
    "        top_wrapper = feats\n",
    "\n",
    "    pmax = 1\n",
    "    \n",
    "    while len(top_wrapper) > 0:\n",
    "        p = []\n",
    "\n",
    "        X_new = X[top_wrapper]\n",
    "        X_new = sm.add_constant(X_new)\n",
    "\n",
    "        model = sm.OLS(y, X_new).fit()\n",
    "\n",
    "        p = pd.Series(model.pvalues.values[1:], index=top_wrapper)\n",
    "\n",
    "        print('Bottom 5 Features:')\n",
    "        print(p.sort_values()[-5:])\n",
    "\n",
    "        pmax = max(p)\n",
    "\n",
    "        feature_pmax = p.idxmax()\n",
    "\n",
    "        if pmax > thresh:\n",
    "            top_wrapper.remove(feature_pmax)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return top_wrapper, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aa4bc4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 5 Features:\n",
      "feature_25    0.860902\n",
      "feature_8     0.890067\n",
      "feature_4     0.966529\n",
      "feature_15    0.976145\n",
      "feature_9     0.995240\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_11    0.854456\n",
      "feature_25    0.855102\n",
      "feature_8     0.890067\n",
      "feature_4     0.965257\n",
      "feature_15    0.976352\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_24    0.705653\n",
      "feature_25    0.806017\n",
      "feature_11    0.853746\n",
      "feature_8     0.889478\n",
      "feature_4     0.967554\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_27    0.605879\n",
      "feature_22    0.678266\n",
      "feature_25    0.808471\n",
      "feature_11    0.856529\n",
      "feature_8     0.883741\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_12    0.573192\n",
      "feature_27    0.612051\n",
      "feature_22    0.680165\n",
      "feature_25    0.825525\n",
      "feature_11    0.877632\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_24    0.548588\n",
      "feature_12    0.561979\n",
      "feature_27    0.598448\n",
      "feature_22    0.685406\n",
      "feature_25    0.822968\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_18    0.539737\n",
      "feature_12    0.544189\n",
      "feature_24    0.574441\n",
      "feature_27    0.576287\n",
      "feature_22    0.716892\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_3     0.472826\n",
      "feature_18    0.493643\n",
      "feature_24    0.557599\n",
      "feature_13    0.559839\n",
      "feature_27    0.610862\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_19    0.440224\n",
      "feature_1     0.455693\n",
      "feature_24    0.495743\n",
      "feature_18    0.525585\n",
      "feature_13    0.640060\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_2     0.400412\n",
      "feature_1     0.430855\n",
      "feature_18    0.455440\n",
      "feature_19    0.467061\n",
      "feature_24    0.536060\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_3     0.335424\n",
      "feature_19    0.363484\n",
      "feature_2     0.402381\n",
      "feature_1     0.510919\n",
      "feature_18    0.545839\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_12    0.225890\n",
      "feature_3     0.354157\n",
      "feature_2     0.420172\n",
      "feature_19    0.432174\n",
      "feature_1     0.448372\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_6     0.202010\n",
      "feature_12    0.207781\n",
      "feature_3     0.361777\n",
      "feature_2     0.401696\n",
      "feature_19    0.456579\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_6     0.211854\n",
      "feature_0     0.215868\n",
      "feature_12    0.243804\n",
      "feature_3     0.325460\n",
      "feature_2     0.456301\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_10    0.101310\n",
      "feature_6     0.110695\n",
      "feature_0     0.118908\n",
      "feature_3     0.329010\n",
      "feature_12    0.357695\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_7     0.048449\n",
      "feature_10    0.058620\n",
      "feature_6     0.125268\n",
      "feature_0     0.139189\n",
      "feature_3     0.412096\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_17    0.005757\n",
      "feature_10    0.011162\n",
      "feature_0     0.014961\n",
      "feature_7     0.035917\n",
      "feature_6     0.122036\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_28    0.000305\n",
      "feature_10    0.002003\n",
      "feature_29    0.003838\n",
      "feature_17    0.020893\n",
      "feature_0     0.026530\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_14    0.000007\n",
      "feature_28    0.000017\n",
      "feature_29    0.000203\n",
      "feature_10    0.000358\n",
      "feature_17    0.010184\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_26    0.000022\n",
      "feature_28    0.000048\n",
      "feature_10    0.000113\n",
      "feature_16    0.000152\n",
      "feature_29    0.000273\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_16    0.000002\n",
      "feature_7     0.000004\n",
      "feature_5     0.000004\n",
      "feature_28    0.000006\n",
      "feature_10    0.000245\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_26    1.339203e-08\n",
      "feature_7     5.808642e-07\n",
      "feature_28    6.455968e-06\n",
      "feature_5     4.058938e-05\n",
      "feature_16    1.726049e-04\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_28    2.753859e-08\n",
      "feature_14    4.870677e-07\n",
      "feature_7     8.275352e-07\n",
      "feature_26    1.846900e-05\n",
      "feature_5     3.359579e-05\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_21    1.931242e-10\n",
      "feature_28    2.418798e-06\n",
      "feature_14    8.207956e-06\n",
      "feature_7     2.896091e-03\n",
      "feature_26    7.477581e-03\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_21    1.061508e-12\n",
      "feature_23    6.169932e-11\n",
      "feature_28    3.983863e-09\n",
      "feature_7     1.173970e-07\n",
      "feature_14    3.960764e-05\n",
      "dtype: float64\n",
      "Bottom 5 Features:\n",
      "feature_20    7.744141e-16\n",
      "feature_7     2.202708e-15\n",
      "feature_21    3.228496e-13\n",
      "feature_23    2.979581e-08\n",
      "feature_28    2.223307e-07\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "top_wrapper, p = wrapper_OLS(X, y, feats=X.columns.tolist(), thresh=10 * 10e-7)\n",
    "top_wrapper = p.nsmallest(top_n).index.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315bda8",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4009678",
   "metadata": {},
   "source": [
    "An extra trees classifier is an ensemble of aggregated decision tree classifiers into a “forest”. Each tree in the forest is given a random sample of k features, each tree then determines the top performing feature by selecting a random cutpoint for each feature, the separation of classes is then evaluated using a statistical criteria, typically Entropy or Gini. The result is multiple decision trees where a random sub set of features for each tree have been ranked. This is useful for feature selection as the feature importance, and therefore the top performing features are automatically determined during the training process. This process can be run multiple times, where the top n features are determined. The features returned from each run can then be compared, once subsequent runs are found to return no new features then the process is ended. This method is useful for filtering large numbers of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "03ccc1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "def extra_trees_classifier(\n",
    "    X: pd.DataFrame,\n",
    "    y,\n",
    "    limit=50,\n",
    "    n_estimators=100,\n",
    "    criterion=\"entropy\",\n",
    "    max_features=3,\n",
    "    extracted_per_run: int = 10,\n",
    "    plot=False,\n",
    "):\n",
    "    top = []\n",
    "    for i in range(limit):\n",
    "        extra_tree_forest = ExtraTreesClassifier(\n",
    "            n_estimators=n_estimators, criterion=criterion, max_features=max_features\n",
    "        )\n",
    "\n",
    "        # Training the model\n",
    "        extra_tree_forest.fit(X, y)\n",
    "\n",
    "        # Computing the importance of each feature\n",
    "        feature_importance = extra_tree_forest.feature_importances_\n",
    "\n",
    "        # Normalizing the individual importances\n",
    "        feature_importance_normalized = np.std(\n",
    "            [tree.feature_importances_ for tree in extra_tree_forest.estimators_],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        feat_importances = pd.Series(feature_importance_normalized, index=X.columns)\n",
    "\n",
    "        nlargest = feat_importances.nlargest(extracted_per_run)\n",
    "        if plot:\n",
    "            nlargest.plot(kind=\"barh\")\n",
    "            plt.show()\n",
    "\n",
    "        top_100 = nlargest.index.values.tolist()\n",
    "\n",
    "        top = list(top)\n",
    "        s1 = len(top)\n",
    "\n",
    "        diff_len = len(set(top_100) - set(top))\n",
    "        top.extend(top_100)\n",
    "        top = set(top)\n",
    "\n",
    "        s2 = len(list(top))\n",
    "\n",
    "        print(f\"Run: {i + 1}, Stored features: {s1}, total features {s2}, new features: {diff_len}\")\n",
    "\n",
    "        if s1 == s2:\n",
    "            break\n",
    "\n",
    "    return list(set(top))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f97da44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, Stored features: 0, total features 10, new features: 10\n",
      "Run: 2, Stored features: 10, total features 11, new features: 1\n",
      "Run: 3, Stored features: 11, total features 11, new features: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature_2',\n",
       " 'feature_26',\n",
       " 'feature_27',\n",
       " 'feature_23',\n",
       " 'feature_6',\n",
       " 'feature_3',\n",
       " 'feature_22',\n",
       " 'feature_13',\n",
       " 'feature_7',\n",
       " 'feature_0',\n",
       " 'feature_20']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_trees_classifier(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8c8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
